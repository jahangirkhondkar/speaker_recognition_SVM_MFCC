{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Initial Training Using an SVM Model (Non-Random Split 80-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1600\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1657\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=2032\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1664\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1033\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=418\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM model...\n",
      "SVM Model Evaluation:\n",
      "Accuracy: 0.96\n",
      "F1 Score: 0.90\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths to the prepared dataset\n",
    "train_dir = r\"E:\\Python_proj\\ML_Project\\musan\\train\"\n",
    "test_dir = r\"E:\\Python_proj\\ML_Project\\musan\\test\"\n",
    "\n",
    "# Function to extract MFCC features\n",
    "def extract_features(file_path, n_mfcc=13):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        mfcc_features = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        return np.mean(mfcc_features.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load data and extract features from a directory\n",
    "def load_data(directory, label):\n",
    "    features, labels = [], []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                mfcc = extract_features(file_path)\n",
    "                if mfcc is not None:\n",
    "                    features.append(mfcc)\n",
    "                    labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "X_train_speech, y_train_speech = load_data(os.path.join(train_dir, 'speech'), label=1)\n",
    "X_train_non_speech, y_train_non_speech = load_data(os.path.join(train_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for training\n",
    "X_train = np.vstack((X_train_speech, X_train_non_speech))\n",
    "y_train = np.hstack((y_train_speech, y_train_non_speech))\n",
    "\n",
    "# Load testing data\n",
    "print(\"Loading testing data...\")\n",
    "X_test_speech, y_test_speech = load_data(os.path.join(test_dir, 'speech'), label=1)\n",
    "X_test_non_speech, y_test_non_speech = load_data(os.path.join(test_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for testing\n",
    "X_test = np.vstack((X_test_speech, X_test_non_speech))\n",
    "y_test = np.hstack((y_test_speech, y_test_non_speech))\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM model\n",
    "print(\"Training SVM model...\")\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"SVM Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Adding a Validation Split (80-10-10 Random Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1600\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1657\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=2032\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1664\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data for splitting into validation and test sets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1033\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=418\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM model...\n",
      "Validation Set Evaluation:\n",
      "Accuracy: 0.97\n",
      "F1 Score: 0.93\n",
      "Test Set Evaluation:\n",
      "Accuracy: 0.95\n",
      "F1 Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths to the dataset directories\n",
    "train_dir = r\"E:\\Python_proj\\ML_Project\\musan\\train\"\n",
    "test_dir = r\"E:\\Python_proj\\ML_Project\\musan\\test\"\n",
    "\n",
    "# Function to extract MFCC features\n",
    "def extract_features(file_path, n_mfcc=13):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        mfcc_features = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        return np.mean(mfcc_features.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load data and extract features from a directory\n",
    "def load_data(directory, label):\n",
    "    features, labels = [], []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                mfcc = extract_features(file_path)\n",
    "                if mfcc is not None:\n",
    "                    features.append(mfcc)\n",
    "                    labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Load training data from the train folder\n",
    "print(\"Loading training data...\")\n",
    "X_train_speech, y_train_speech = load_data(os.path.join(train_dir, 'speech'), label=1)\n",
    "X_train_non_speech, y_train_non_speech = load_data(os.path.join(train_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for training\n",
    "X_train = np.vstack((X_train_speech, X_train_non_speech))\n",
    "y_train = np.hstack((y_train_speech, y_train_non_speech))\n",
    "\n",
    "# Load all test data from the test folder\n",
    "print(\"Loading test data for splitting into validation and test sets...\")\n",
    "X_test_speech, y_test_speech = load_data(os.path.join(test_dir, 'speech'), label=1)\n",
    "X_test_non_speech, y_test_non_speech = load_data(os.path.join(test_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech test data\n",
    "X_test_all = np.vstack((X_test_speech, X_test_non_speech))\n",
    "y_test_all = np.hstack((y_test_speech, y_test_non_speech))\n",
    "\n",
    "# Split the 20% test data into 10% validation and 10% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_all, y_test_all, test_size=0.5, random_state=42, stratify=y_test_all)\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the SVM model on the training set\n",
    "print(\"Training SVM model...\")\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_val_pred = svm_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(f\"Accuracy: {val_accuracy:.2f}\")\n",
    "print(f\"F1 Score: {val_f1:.2f}\")\n",
    "\n",
    "# Final evaluation on the test set\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(f\"Accuracy: {test_accuracy:.2f}\")\n",
    "print(f\"F1 Score: {test_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Filtering the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loading testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1024\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=512\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM model...\n",
      "SVM Model Classification:\n",
      "Accuracy: 0.93\n",
      "F1 Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths to the prepared dataset\n",
    "train_dir = r\"E:\\Python_proj\\ML_Project\\musan\\train_filtered\"\n",
    "test_dir = r\"E:\\Python_proj\\ML_Project\\musan\\test_filtered\"\n",
    "\n",
    "# Function to extract MFCC features\n",
    "def extract_features(file_path, n_mfcc=13):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        mfcc_features = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        return np.mean(mfcc_features.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load data and extract features from a directory\n",
    "def load_data(directory, label):\n",
    "    features, labels = [], []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                mfcc = extract_features(file_path)\n",
    "                if mfcc is not None:\n",
    "                    features.append(mfcc)\n",
    "                    labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "X_train_speech, y_train_speech = load_data(os.path.join(train_dir, 'speech'), label=1)\n",
    "X_train_non_speech, y_train_non_speech = load_data(os.path.join(train_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for training\n",
    "X_train = np.vstack((X_train_speech, X_train_non_speech))\n",
    "y_train = np.hstack((y_train_speech, y_train_non_speech))\n",
    "\n",
    "# Load testing data\n",
    "print(\"Loading testing data...\")\n",
    "X_test_speech, y_test_speech = load_data(os.path.join(test_dir, 'speech'), label=1)\n",
    "X_test_non_speech, y_test_non_speech = load_data(os.path.join(test_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for testing\n",
    "X_test = np.vstack((X_test_speech, X_test_non_speech))\n",
    "y_test = np.hstack((y_test_speech, y_test_non_speech))\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM model\n",
    "print(\"Training SVM model...\")\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"SVM Model Classification:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Comparing with Hidden Markov Model (HMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1600\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1657\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=2032\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1664\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1033\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=418\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM Model Performance:\n",
      "Accuracy: 0.70\n",
      "F1 Score: 0.58\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from librosa.feature import mfcc\n",
    "from librosa import load\n",
    "\n",
    "def extract_features(file_path, n_mfcc=13):\n",
    "    try:\n",
    "        y, sr = load(file_path, sr=None)\n",
    "        mfcc_features = mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        return np.mean(mfcc_features.T, axis=0)  # Return averaged MFCCs\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_data(directory, label):\n",
    "    features, labels = [], []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                mfcc = extract_features(file_path)\n",
    "                if mfcc is not None:\n",
    "                    features.append(mfcc)\n",
    "                    labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Paths to training and testing directories\n",
    "train_dir = r\"E:\\Python_proj\\ML_Project\\musan\\train\"\n",
    "test_dir = r\"E:\\Python_proj\\ML_Project\\musan\\test\"\n",
    "\n",
    "# Load training data\n",
    "X_train_speech, y_train_speech = load_data(os.path.join(train_dir, 'speech'), label=1)\n",
    "X_train_non_speech, y_train_non_speech = load_data(os.path.join(train_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech training data\n",
    "X_train = np.vstack((X_train_speech, X_train_non_speech))\n",
    "y_train = np.hstack((y_train_speech, y_train_non_speech))\n",
    "\n",
    "# Load testing data\n",
    "X_test_speech, y_test_speech = load_data(os.path.join(test_dir, 'speech'), label=1)\n",
    "X_test_non_speech, y_test_non_speech = load_data(os.path.join(test_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech testing data\n",
    "X_test = np.vstack((X_test_speech, X_test_non_speech))\n",
    "y_test = np.hstack((y_test_speech, y_test_non_speech))\n",
    "\n",
    "# Train HMM for speech\n",
    "speech_hmm = GaussianHMM(n_components=5, covariance_type='full', random_state=42, n_iter=100)\n",
    "speech_hmm.fit(X_train_speech)\n",
    "\n",
    "# Train HMM for non-speech\n",
    "non_speech_hmm = GaussianHMM(n_components=5, covariance_type='full', random_state=42, n_iter=100)\n",
    "non_speech_hmm.fit(X_train_non_speech)\n",
    "\n",
    "y_pred = []\n",
    "for i, x in enumerate(X_test):\n",
    "    # Compute log-likelihoods for speech and non-speech HMMs\n",
    "    speech_score = speech_hmm.score([x])\n",
    "    non_speech_score = non_speech_hmm.score([x])\n",
    "    \n",
    "    # Assign label based on higher score\n",
    "    if speech_score > non_speech_score:\n",
    "        y_pred.append(1)  # Speech\n",
    "    else:\n",
    "        y_pred.append(0)  # Non-speech\n",
    "\n",
    "# Convert predictions to numpy array\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"HMM Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Applying t-Test Before and After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1600\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1657\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=2032\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1664\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1033\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=418\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing t-test on raw data before training...\n",
      "T-test before training - t-statistic: -2.79, p-value: 5.48e-03\n",
      "Training SVM model...\n",
      "SVM Model Evaluation:\n",
      "Accuracy: 0.96\n",
      "F1 Score: 0.90\n",
      "Performing t-test on predicted labels after training...\n",
      "T-test after training - t-statistic: 0.73, p-value: 4.68e-01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Paths to the prepared dataset\n",
    "train_dir = r\"E:\\Python_proj\\ML_Project\\musan\\train\"\n",
    "test_dir = r\"E:\\Python_proj\\ML_Project\\musan\\test\"\n",
    "\n",
    "# Function to extract MFCC features\n",
    "def extract_features(file_path, n_mfcc=13):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        mfcc_features = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        return np.mean(mfcc_features.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load data and extract features from a directory\n",
    "def load_data(directory, label):\n",
    "    features, labels = [], []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                mfcc = extract_features(file_path)\n",
    "                if mfcc is not None:\n",
    "                    features.append(mfcc)\n",
    "                    labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "X_train_speech, y_train_speech = load_data(os.path.join(train_dir, 'speech'), label=1)\n",
    "X_train_non_speech, y_train_non_speech = load_data(os.path.join(train_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for training\n",
    "X_train = np.vstack((X_train_speech, X_train_non_speech))\n",
    "y_train = np.hstack((y_train_speech, y_train_non_speech))\n",
    "\n",
    "# Load testing data\n",
    "print(\"Loading testing data...\")\n",
    "X_test_speech, y_test_speech = load_data(os.path.join(test_dir, 'speech'), label=1)\n",
    "X_test_non_speech, y_test_non_speech = load_data(os.path.join(test_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for testing\n",
    "X_test = np.vstack((X_test_speech, X_test_non_speech))\n",
    "y_test = np.hstack((y_test_speech, y_test_non_speech))\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Ensure equal lengths for t-test\n",
    "min_length = min(len(X_train), len(X_test))\n",
    "X_train_sample = X_train[:min_length, 0]  # Take the first feature for comparison\n",
    "X_test_sample = X_test[:min_length, 0]   # Take the first feature for comparison\n",
    "\n",
    "# Conduct t-test on raw data before training\n",
    "print(\"Performing t-test on raw data before training...\")\n",
    "t_stat, p_value = ttest_rel(X_train_sample, X_test_sample)\n",
    "print(f\"T-test before training - t-statistic: {t_stat:.2f}, p-value: {p_value:.2e}\")\n",
    "\n",
    "# Train SVM model\n",
    "print(\"Training SVM model...\")\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"SVM Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Ensure equal lengths for t-test after training\n",
    "y_test_sample = y_test[:min_length]\n",
    "y_pred_sample = y_pred[:min_length]\n",
    "\n",
    "# Conduct t-test on predicted labels after training\n",
    "print(\"Performing t-test on predicted labels after training...\")\n",
    "t_stat_post, p_value_post = ttest_rel(y_test_sample, y_pred_sample)\n",
    "print(f\"T-test after training - t-statistic: {t_stat_post:.2f}, p-value: {p_value_post:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1600\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1657\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=2032\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1664\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1033\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=418\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PCA...\n",
      "Training SVM model with PCA...\n",
      "SVM Model Evaluation with PCA:\n",
      "Accuracy: 0.95\n",
      "F1 Score: 0.88\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Paths to the prepared dataset\n",
    "train_dir = r\"E:\\Python_proj\\ML_Project\\musan\\train\"\n",
    "test_dir = r\"E:\\Python_proj\\ML_Project\\musan\\test\"\n",
    "\n",
    "# Function to extract MFCC features\n",
    "def extract_features(file_path, n_mfcc=13):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        mfcc_features = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        return np.mean(mfcc_features.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load data and extract features from a directory\n",
    "def load_data(directory, label):\n",
    "    features, labels = [], []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                mfcc = extract_features(file_path)\n",
    "                if mfcc is not None:\n",
    "                    features.append(mfcc)\n",
    "                    labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "X_train_speech, y_train_speech = load_data(os.path.join(train_dir, 'speech'), label=1)\n",
    "X_train_non_speech, y_train_non_speech = load_data(os.path.join(train_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for training\n",
    "X_train = np.vstack((X_train_speech, X_train_non_speech))\n",
    "y_train = np.hstack((y_train_speech, y_train_non_speech))\n",
    "\n",
    "# Load testing data\n",
    "print(\"Loading testing data...\")\n",
    "X_test_speech, y_test_speech = load_data(os.path.join(test_dir, 'speech'), label=1)\n",
    "X_test_non_speech, y_test_non_speech = load_data(os.path.join(test_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for testing\n",
    "X_test = np.vstack((X_test_speech, X_test_non_speech))\n",
    "y_test = np.hstack((y_test_speech, y_test_non_speech))\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "print(\"Applying PCA...\")\n",
    "pca = PCA(n_components=10)  # Reduce to 10 principal components\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train SVM model\n",
    "print(\"Training SVM model with PCA...\")\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm_model.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"SVM Model Evaluation with PCA:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Adding Zero Crossing Rate (ZCR) to MFCC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1600\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1657\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=2032\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1664\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1033\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=418\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM model with MFCC and ZCR...\n",
      "SVM Model Evaluation with MFCC and ZCR:\n",
      "Accuracy: 0.96\n",
      "F1 Score: 0.91\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Paths to the prepared dataset\n",
    "train_dir = r\"E:\\Python_proj\\ML_Project\\musan\\train\"\n",
    "test_dir = r\"E:\\Python_proj\\ML_Project\\musan\\test\"\n",
    "\n",
    "# Function to extract MFCC features and Zero Crossing Rate\n",
    "def extract_features(file_path, n_mfcc=13):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfcc_features = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        mfcc_mean = np.mean(mfcc_features.T, axis=0)\n",
    "\n",
    "        # Extract Zero Crossing Rate (ZCR)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        zcr_mean = np.mean(zcr)\n",
    "\n",
    "        # Combine MFCC and ZCR\n",
    "        features = np.append(mfcc_mean, zcr_mean)\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load data and extract features from a directory\n",
    "def load_data(directory, label):\n",
    "    features, labels = [], []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                feature = extract_features(file_path)\n",
    "                if feature is not None:\n",
    "                    features.append(feature)\n",
    "                    labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "X_train_speech, y_train_speech = load_data(os.path.join(train_dir, 'speech'), label=1)\n",
    "X_train_non_speech, y_train_non_speech = load_data(os.path.join(train_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for training\n",
    "X_train = np.vstack((X_train_speech, X_train_non_speech))\n",
    "y_train = np.hstack((y_train_speech, y_train_non_speech))\n",
    "\n",
    "# Load testing data\n",
    "print(\"Loading testing data...\")\n",
    "X_test_speech, y_test_speech = load_data(os.path.join(test_dir, 'speech'), label=1)\n",
    "X_test_non_speech, y_test_non_speech = load_data(os.path.join(test_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for testing\n",
    "X_test = np.vstack((X_test_speech, X_test_non_speech))\n",
    "y_test = np.hstack((y_test_speech, y_test_non_speech))\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM model\n",
    "print(\"Training SVM model with MFCC and ZCR...\")\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"SVM Model Evaluation with MFCC and ZCR:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Modifying Window and Hop Length (20 ms and 10 ms, respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\feature\\spectral.py:2143: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading testing data...\n",
      "Training SVM model with updated window size and hop length...\n",
      "SVM Model Evaluation (Standard Windowing Parameters):\n",
      "Accuracy: 0.97\n",
      "F1 Score: 0.91\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths to the prepared dataset\n",
    "train_dir = r\"E:\\Python_proj\\ML_Project\\musan\\train\"\n",
    "test_dir = r\"E:\\Python_proj\\ML_Project\\musan\\test\"\n",
    "\n",
    "# Function to extract MFCC features with updated window size and hop length\n",
    "def extract_features(file_path, n_mfcc=13, n_fft=320, hop_length=160):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "        # Extract MFCC features with standard 20ms window size and 10ms hop length\n",
    "        mfcc_features = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        return np.mean(mfcc_features.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load data and extract features from a directory\n",
    "def load_data(directory, label):\n",
    "    features, labels = [], []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                feature = extract_features(file_path)\n",
    "                if feature is not None:\n",
    "                    features.append(feature)\n",
    "                    labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Load training data (80/20 non-random split)\n",
    "print(\"Loading training data...\")\n",
    "X_train_speech, y_train_speech = load_data(os.path.join(train_dir, 'speech'), label=1)\n",
    "X_train_non_speech, y_train_non_speech = load_data(os.path.join(train_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for training\n",
    "X_train = np.vstack((X_train_speech, X_train_non_speech))\n",
    "y_train = np.hstack((y_train_speech, y_train_non_speech))\n",
    "\n",
    "# Load testing data\n",
    "print(\"Loading testing data...\")\n",
    "X_test_speech, y_test_speech = load_data(os.path.join(test_dir, 'speech'), label=1)\n",
    "X_test_non_speech, y_test_non_speech = load_data(os.path.join(test_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for testing\n",
    "X_test = np.vstack((X_test_speech, X_test_non_speech))\n",
    "y_test = np.hstack((y_test_speech, y_test_non_speech))\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM model\n",
    "print(\"Training SVM model with updated window size and hop length...\")\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"SVM Model Evaluation (Standard Windowing Parameters):\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Combined Optimizations for Final Evaluation (Non-Random Split 80/20) with t-Test and ZCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\feature\\spectral.py:2143: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading testing data...\n",
      "Performing t-test on raw data before training...\n",
      "T-test before training - t-statistic: -3.07, p-value: 2.32e-03\n",
      "Training SVM model with MFCC and ZCR...\n",
      "SVM Model Evaluation with MFCC and ZCR (Non-Random Split):\n",
      "Accuracy: 0.97\n",
      "F1 Score: 0.91\n",
      "Performing t-test on predicted labels after training...\n",
      "T-test after training - t-statistic: 1.07, p-value: 2.86e-01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Paths to the prepared dataset\n",
    "train_dir = r\"E:\\Python_proj\\ML_Project\\musan\\train\"\n",
    "test_dir = r\"E:\\Python_proj\\ML_Project\\musan\\test\"\n",
    "\n",
    "# Function to extract MFCC features and Zero Crossing Rate\n",
    "def extract_features(file_path, n_mfcc=13, n_fft=320, hop_length=160):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "        # Extract MFCC features with updated window length (20ms) and 50% overlap (hop length)\n",
    "        mfcc_features = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        mfcc_mean = np.mean(mfcc_features.T, axis=0)\n",
    "\n",
    "        # Extract Zero Crossing Rate (ZCR)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y, hop_length=hop_length)\n",
    "        zcr_mean = np.mean(zcr)\n",
    "\n",
    "        # Combine MFCC and ZCR\n",
    "        features = np.append(mfcc_mean, zcr_mean)\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load data and extract features from a directory\n",
    "def load_data(directory, label):\n",
    "    features, labels = [], []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                feature = extract_features(file_path)\n",
    "                if feature is not None:\n",
    "                    features.append(feature)\n",
    "                    labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Load training data (80-20 split non-random)\n",
    "print(\"Loading training data...\")\n",
    "X_train_speech, y_train_speech = load_data(os.path.join(train_dir, 'speech'), label=1)\n",
    "X_train_non_speech, y_train_non_speech = load_data(os.path.join(train_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for training\n",
    "X_train = np.vstack((X_train_speech, X_train_non_speech))\n",
    "y_train = np.hstack((y_train_speech, y_train_non_speech))\n",
    "\n",
    "# Load testing data\n",
    "print(\"Loading testing data...\")\n",
    "X_test_speech, y_test_speech = load_data(os.path.join(test_dir, 'speech'), label=1)\n",
    "X_test_non_speech, y_test_non_speech = load_data(os.path.join(test_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for testing\n",
    "X_test = np.vstack((X_test_speech, X_test_non_speech))\n",
    "y_test = np.hstack((y_test_speech, y_test_non_speech))\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Ensure equal lengths for t-test\n",
    "min_length = min(len(X_train), len(X_test))\n",
    "X_train_sample = X_train[:min_length, 0]  # Take the first feature for comparison\n",
    "X_test_sample = X_test[:min_length, 0]   # Take the first feature for comparison\n",
    "\n",
    "# Conduct t-test on raw data before training\n",
    "print(\"Performing t-test on raw data before training...\")\n",
    "t_stat, p_value = ttest_rel(X_train_sample, X_test_sample)\n",
    "print(f\"T-test before training - t-statistic: {t_stat:.2f}, p-value: {p_value:.2e}\")\n",
    "\n",
    "# Train SVM model\n",
    "print(\"Training SVM model with MFCC and ZCR...\")\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"SVM Model Evaluation with MFCC and ZCR (Non-Random Split):\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Ensure equal lengths for t-test after training\n",
    "y_test_sample = y_test[:min_length]\n",
    "y_pred_sample = y_pred[:min_length]\n",
    "\n",
    "# Conduct t-test on predicted labels after training\n",
    "print(\"Performing t-test on predicted labels after training...\")\n",
    "t_stat_post, p_value_post = ttest_rel(y_test_sample, y_pred_sample)\n",
    "print(f\"T-test after training - t-statistic: {t_stat_post:.2f}, p-value: {p_value_post:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10: Combined Optimizations for Final Evaluation (Random Split 80/10/10) with t-Test and ZCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\feature\\spectral.py:2143: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing t-test on raw data before training...\n",
      "T-test before training - t-statistic: 2.01, p-value: 4.65e-02\n",
      "Training SVM model with MFCC and ZCR...\n",
      "SVM Model Evaluation with MFCC and ZCR (Random Split):\n",
      "Accuracy: 0.99\n",
      "F1 Score: 0.97\n",
      "Performing t-test on predicted labels after training...\n",
      "T-test after training - t-statistic: 0.00, p-value: 1.00e+00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Paths to the prepared dataset\n",
    "train_dir = r\"E:\\Python_proj\\ML_Project\\musan\\train\"\n",
    "test_dir = r\"E:\\Python_proj\\ML_Project\\musan\\test\"\n",
    "\n",
    "# Function to extract MFCC features and Zero Crossing Rate\n",
    "def extract_features(file_path, n_mfcc=13, n_fft=320, hop_length=160):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "        # Extract MFCC features with updated window length (20ms) and 50% overlap (hop length)\n",
    "        mfcc_features = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        mfcc_mean = np.mean(mfcc_features.T, axis=0)\n",
    "\n",
    "        # Extract Zero Crossing Rate (ZCR)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y, hop_length=hop_length)\n",
    "        zcr_mean = np.mean(zcr)\n",
    "\n",
    "        # Combine MFCC and ZCR\n",
    "        features = np.append(mfcc_mean, zcr_mean)\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load data and extract features from a directory\n",
    "def load_data(directory, label):\n",
    "    features, labels = [], []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                feature = extract_features(file_path)\n",
    "                if feature is not None:\n",
    "                    features.append(feature)\n",
    "                    labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Load and combine data\n",
    "print(\"Loading data...\")\n",
    "X_speech, y_speech = load_data(os.path.join(train_dir, 'speech'), label=1)\n",
    "X_non_speech, y_non_speech = load_data(os.path.join(train_dir, 'non_speech'), label=0)\n",
    "\n",
    "X = np.vstack((X_speech, X_non_speech))\n",
    "y = np.hstack((y_speech, y_non_speech))\n",
    "\n",
    "# Split data into training, validation, and testing (80-10-10 random split)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Ensure equal lengths for t-test\n",
    "min_length = min(len(X_train), len(X_test))\n",
    "X_train_sample = X_train[:min_length, 0]  # Take the first feature for comparison\n",
    "X_test_sample = X_test[:min_length, 0]   # Take the first feature for comparison\n",
    "\n",
    "# Conduct t-test on raw data before training\n",
    "print(\"Performing t-test on raw data before training...\")\n",
    "t_stat, p_value = ttest_rel(X_train_sample, X_test_sample)\n",
    "print(f\"T-test before training - t-statistic: {t_stat:.2f}, p-value: {p_value:.2e}\")\n",
    "\n",
    "# Train SVM model\n",
    "print(\"Training SVM model with MFCC and ZCR...\")\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"SVM Model Evaluation with MFCC and ZCR (Random Split):\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Ensure equal lengths for t-test after training\n",
    "y_test_sample = y_test[:min_length]\n",
    "y_pred_sample = y_pred[:min_length]\n",
    "\n",
    "# Conduct t-test on predicted labels after training\n",
    "print(\"Performing t-test on predicted labels after training...\")\n",
    "t_stat_post, p_value_post = ttest_rel(y_test_sample, y_pred_sample)\n",
    "print(f\"T-test after training - t-statistic: {t_stat_post:.2f}, p-value: {p_value_post:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 11: K-fold Cross-Validation (5-fold) in Step 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1600\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1657\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=2032\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1664\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data for splitting into validation and test sets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1033\n",
      "  warnings.warn(\n",
      "c:\\Users\\JAHANGIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=418\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing t-test on raw data before training...\n",
      "T-test before training - t-statistic: -5.02, p-value: 1.12e-06\n",
      "Performing 5-fold cross-validation on the training data...\n",
      "Cross-Validation Accuracy Scores: [0.86068111 0.88235294 0.99068323 0.93167702 0.97204969]\n",
      "Mean Cross-Validation Accuracy: 0.93\n",
      "Cross-Validation F1 Scores: [0.62184874 0.63461538 0.97777778 0.84931507 0.93706294]\n",
      "Mean Cross-Validation F1 Score: 0.80\n",
      "Training SVM model on the full training set...\n",
      "Validation Set Evaluation:\n",
      "Accuracy: 0.97\n",
      "F1 Score: 0.93\n",
      "Test Set Evaluation:\n",
      "Accuracy: 0.96\n",
      "F1 Score: 0.89\n",
      "Performing t-test on predicted labels after training...\n",
      "T-test after training - t-statistic: 1.00, p-value: 3.19e-01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Paths to the dataset directories\n",
    "train_dir = r\"E:\\Python_proj\\ML_Project\\musan\\train\"\n",
    "test_dir = r\"E:\\Python_proj\\ML_Project\\musan\\test\"\n",
    "\n",
    "# Function to extract MFCC features and Zero Crossing Rate\n",
    "def extract_features(file_path, n_mfcc=13):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfcc_features = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        mfcc_mean = np.mean(mfcc_features.T, axis=0)\n",
    "\n",
    "        # Extract Zero Crossing Rate (ZCR)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        zcr_mean = np.mean(zcr)\n",
    "\n",
    "        # Combine MFCC and ZCR\n",
    "        features = np.append(mfcc_mean, zcr_mean)\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load data and extract features from a directory\n",
    "def load_data(directory, label):\n",
    "    features, labels = [], []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                feature = extract_features(file_path)\n",
    "                if feature is not None:\n",
    "                    features.append(feature)\n",
    "                    labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Load training data from the train folder\n",
    "print(\"Loading training data...\")\n",
    "X_train_speech, y_train_speech = load_data(os.path.join(train_dir, 'speech'), label=1)\n",
    "X_train_non_speech, y_train_non_speech = load_data(os.path.join(train_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech data for training\n",
    "X_train = np.vstack((X_train_speech, X_train_non_speech))\n",
    "y_train = np.hstack((y_train_speech, y_train_non_speech))\n",
    "\n",
    "# Load all test data from the test folder\n",
    "print(\"Loading test data for splitting into validation and test sets...\")\n",
    "X_test_speech, y_test_speech = load_data(os.path.join(test_dir, 'speech'), label=1)\n",
    "X_test_non_speech, y_test_non_speech = load_data(os.path.join(test_dir, 'non_speech'), label=0)\n",
    "\n",
    "# Combine speech and non-speech test data\n",
    "X_test_all = np.vstack((X_test_speech, X_test_non_speech))\n",
    "y_test_all = np.hstack((y_test_speech, y_test_non_speech))\n",
    "\n",
    "# Split the 20% test data into 10% validation and 10% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_all, y_test_all, test_size=0.5, random_state=42, stratify=y_test_all)\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Ensure equal lengths for t-test\n",
    "min_length = min(len(X_train), len(X_test))\n",
    "X_train_sample = X_train[:min_length, 0]  # Take the first feature for comparison\n",
    "X_test_sample = X_test[:min_length, 0]   # Take the first feature for comparison\n",
    "\n",
    "# Conduct t-test on raw data before training\n",
    "print(\"Performing t-test on raw data before training...\")\n",
    "t_stat, p_value = ttest_rel(X_train_sample, X_test_sample)\n",
    "print(f\"T-test before training - t-statistic: {t_stat:.2f}, p-value: {p_value:.2e}\")\n",
    "\n",
    "# Train SVM model using 5-fold cross-validation\n",
    "print(\"Performing 5-fold cross-validation on the training data...\")\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "accuracy_scores = cross_val_score(svm_model, X_train, y_train, cv=5, scoring=accuracy_scorer)\n",
    "f1_scores = cross_val_score(svm_model, X_train, y_train, cv=5, scoring=f1_scorer)\n",
    "\n",
    "print(f\"Cross-Validation Accuracy Scores: {accuracy_scores}\")\n",
    "print(f\"Mean Cross-Validation Accuracy: {np.mean(accuracy_scores):.2f}\")\n",
    "print(f\"Cross-Validation F1 Scores: {f1_scores}\")\n",
    "print(f\"Mean Cross-Validation F1 Score: {np.mean(f1_scores):.2f}\")\n",
    "\n",
    "# Train the SVM model on the full training set\n",
    "print(\"Training SVM model on the full training set...\")\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_val_pred = svm_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(f\"Accuracy: {val_accuracy:.2f}\")\n",
    "print(f\"F1 Score: {val_f1:.2f}\")\n",
    "\n",
    "# Final evaluation on the test set\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(f\"Accuracy: {test_accuracy:.2f}\")\n",
    "print(f\"F1 Score: {test_f1:.2f}\")\n",
    "\n",
    "# Ensure equal lengths for t-test after training\n",
    "y_test_sample = y_test[:min_length]\n",
    "y_pred_sample = y_test_pred[:min_length]\n",
    "\n",
    "# Conduct t-test on predicted labels after training\n",
    "print(\"Performing t-test on predicted labels after training...\")\n",
    "t_stat_post, p_value_post = ttest_rel(y_test_sample, y_pred_sample)\n",
    "print(f\"T-test after training - t-statistic: {t_stat_post:.2f}, p-value: {p_value_post:.2e}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "AudioCategorization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
